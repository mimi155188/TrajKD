# TrajKD: Distilling Knowledge via Adaptive Trajectory Curriculum and Dynamic Weighting

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![ICPR 2026](https://img.shields.io/badge/ICPR-2026-blue.svg)](http://icpr2026.org/) **Official PyTorch Implementation** for the paper "TrajKD: Distilling Knowledge via Adaptive Trajectory Curriculum and Dynamic Weighting".

> **Abstract:** Existing Knowledge Distillation (KD) paradigms typically rely on fully converged teachers... (è¿™é‡Œç²˜è´´ä½ çš„å®Œæ•´æ‘˜è¦) ... Extensive experiments on CIFAR-100 and ImageNet demonstrate that TrajKD achieves highly competitive performance against representative methods.

## ğŸš€ Status: Coming Soon

**The full source code and pre-trained models will be released in this repository upon the acceptance of the paper.** We are currently organizing the code and documentation to ensure reproducibility.

## ğŸ–¼ï¸ Framework Overview

![Framework](path/to/your/framework_image.png) 
*(If possible, upload your framework figure here, it looks very professional)*

## ğŸ“… Planned Features
- [ ] Complete training code for CIFAR-100 & ImageNet
- [ ] Pre-trained Teacher & Student checkpoints
- [ ] Evaluation scripts
- [ ] Trajectory sampling visualization tools

## Citation

If you find this project useful, please cite our paper:

```bibtex
@inproceedings{jiu2026trajkd,
  title={TrajKD: Distilling Knowledge via Adaptive Trajectory Curriculum and Dynamic Weighting},
  author={Jiu, Mingyuan and Guo, Mi and Wu, Ziyi and Li, Jiahao and Li, Qian and Zhao, Hongru and Xu, Mingliang},
  booktitle={International Conference on Pattern Recognition (ICPR)},
  year={2026}
}
